# Class Notes- 10/6

## Independence of 2 RV
Def: $x_1 ... x_n$ are independent RV if (for any subset $a_1$ to $a_r$) 

for DRV $P(x_{a_1}, .. x_{a_r}) = P(x_{a_1}) * ... *P(x_{a_r})$

for CRV $f(x_{a_1}, .. x_{a_r}) = f(x_{a_1}) * ... *f(x_{a_r})$

---
## Multinomial Distribution
Model: n independent trials with replacement with $r$ possible outcomes each. $P_i$ = prob of getting ith outcome $i = 1, ..., r$.

$X_i$ = # times the ith outcome appears

The jpmf of $X_1 ,... X_r$ has a multinomial distribution. 

$$P(X_1, \cdots , X_r) = \frac{n!}{X_1!X_2! \cdots X_r!}P_1^{X_1}P_2^{X_2} \cdots P_r^{X_r}$$ 

Divide by the number of permutations of each element to remove permutations from the total number of ways the elements can be arranged/

---
## Expected Value for Functions of 2 RV
$X, Y$ are RV and $Z = h(X,Y)$.

Remeber that the expected value can be thought of as a weighted average so the same idea extends to functions of 2 RV:

DRV: 
$$E(Z) = \sum_X \sum_Y h(X,Y)P(X,Y)$$

CRV: 
$$E(Z) = \int_{-\infin}^{\infin}\int_{-\infin}^{\infin} h(x,y) dy dx$$

Note: You can change the order of summation/integration when the series is convergent (i dont really understand why)

Propositions:
1) $E(aX + Y) = aE(X) + E(Y)$
2) If $X,Y$ are independent, then $E(X,Y) = E(X)E(Y)$

A counterexample s can be found such that the reverse of 2) does not hold meaning $E(X,Y) = E(X)E(Y)$ does not imply independence of $X$ and $Y$. 


Proof of 1)
$$
\begin{aligned}
 E(aX+Y) &= \sum_X \sum_Y (aX+Y)P(X,Y) \\
    &= a \sum_X \sum_Y X P(X,Y) + \sum_X \sum_Y YP(X,Y) \\
    &= a \sum_X  X \sum_Y P(X,Y) + \sum_Y Y \sum_X P(X,Y) \\
    &= a \sum_X  X \sum_Y P_X(X) + \sum_Y Y \sum_X P_Y(Y) \\
    &= aE(X) + E(Y)
\end{aligned}
$$


 ## Covariance
 DEF: $X,Y$ are RV. Then $Cov(X,Y) = E((x- \mu_x)(Y-\mu_y)) = E(X,Y) - E(X)E(Y)$

### Interpretations
- Covariance measures how much $X$ and $Y$ are spread since $Cov(X,X) = \sigma^2$
- How much are $X$ and $Y$ related
- How much $X$ and $Y$ fail to be independent 
    - $Cov(X,Y) = 0$ does not imply independence as demonstrated by the counterexample to proposition 2 above.
- How linear is the relation between $X$ and $Y$

### Examples
INSERT GRAPH of POINTS positively sloping up

This distribution of points results in a large positive covariance

Highly related in normal proportional way

INSERT GRAPH of Points negatively sloping down

This distribution of points results in a large negative covariance.
This can be interpreted as when $X$ increases, $Y$ decreases and when $X$ decresases, $Y$ increases.

Highly related in inverse way

INSERT GRAPH of Circle of points

This distribution will have a covariance close to 0

### Properties
$$Cov(aX,Z) = aCov(X,Z)$$

This implies $Cov(aX,aX) = a^2Cov(X,X)$.

$$Cov(aX + bY, Z) =  aCov(X,Z) + bCov(Y,Z)$$